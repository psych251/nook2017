---
title: "Replication of A Linguistic Signature of Psychological Distancing in Emotion Regulation by Erik C. Nook, Jessica L. Schleider, and Leah H. Somerville (2017, Journal of Experimental Psychology: General)"
author: "Hannah E. Marshall (hannah21@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

Currently, my research investigates young children's implicit understanding of linguistic politeness. I envisage my future work focusing more broadly on language in social contexts, principally on how children and adolescents use language to construct and reconstruct interpersonal relationships. Nook et al. (2017) explore bidirectional relations between emotion regulation and linguistic signatures of psychological distancing. Emotion regulation plays a crucial role in the way children and adolescents perceive and relate to their environment, and is fundamental to developing successful relationships, which is why I deem it relevant to my research interests.

Study 1 assessed (1) whether people spontaneously distance their language when regulating emotions and (2) whether the tendency to use more psychologically distant language when regulating emotions is associated with more successful regulation; it found that emotion regulation increased linguistic markers of social and temporal distance, and participants who showed greater linguistic distancing were more successful regulators. These are the findings I seek to replicate.

The original researchers adapted Ochsner, Bunge, Gross, & Gabrieli's (2002) emotion regulation paradigm for use on Amazon Mechanical Turk (mTurk). The original stimuli consisted of three lists of 20 neutral and negative images from the Open Affective Standardized Image Set (OASIS; Kurdi, Lozano, & Banaji, 2016). 120 MTurk participants saw the cue word "LOOK" or "CHANGE" above an image for 30 seconds, which indicated whether they should react naturally or regulate their emotions by reappraising the meaning of the image, respectively. Participants were instructed to transcribe what they were thinking and feeling into a textbox. The original researchers used Pennebaker's Linguistic Inquiry and Word Count (LIWC) program to analyze the text entries for each trial (Pennebaker, Chung, Ireland, Gonzales, & Booth, 2007).

The only anticipated challenges include compiling the stimuli and becoming familiar with mTurk and the LIWC program.

Link to project repository: https://github.com/psych251/nook2017

Link to original paper: https://github.com/psych251/nook2017/blob/master/original_paper/nook2017.pdf

## Methods

### Power Analysis

A power analysis in G*Power indicated that 63 participants were required to observe a within-subjects emotion regulation effect (estimated d = 0.36) at p < 0.05 and 80% power. As per the original author's power analysis, the estimated effect size was based on a 2012 emotion regulation meta-analysis (Webb, Miles, & Sheeran).

To observe the same effect with 90% power, 84 participants are required. To observe the same effect with 95% power, 103 participants are required.

Due to funding limitations, we may only be able to run 60 participants. If this replication is run with 60 participants, it will be able to detect the same effect with 78% power; however, the final power may be less than 78% after accounting for exclusions.

### Planned Sample

Like the original Nook et al. study, our replication will sample from mTurk workers located in the United States who have at least a 95% task approval rate.

The original researchers "excluded eight participants who consistently progressed through trials without writing for a full 30 s and five who wrote about topics other than the images". Our replication will not allow participants to submit their responses before 20 seconds has elapsed. Responses will be automatically submitted after 1 minute. We will exclude participants who write about topics other than the images.

To ensure compliance with federal minimum wage, participants will be paid $1.50.

### Materials

The original researchers "assembled three lists of 20 images from the Open Affective Standardized Image Set (OASIS; Kurdi, Lozano, & Banaji, 2016). One list included only neutral images [...] The other two lists were both negative, and they were matched for valence [...] and arousal". 

Participants were shown the 60 images, each with "the cue word 'LOOK' or the cue word 'CHANGE' above [...] for 30 s." Participants were told that the cue word 'LOOK' meant they should look at the picture and let themselves feel whatever the image makes them feel. "The cue word 'CHANGE' indicated that they should regulate their emotions by reappraising the meaning of the image (Gross, 1998, 2015). Critically, participants were not instructed to reappraise the image by imagining it as far away from them. Instead, they were instructed to reinterpret the meaning of the image to make it less negative (e.g., imagine that the objects are fake or that something good is about to happen)."

"This design divided trials into three conditions: (a) look negative, (b) reappraise negative, and (c) look neutral."

Our replication will follow the original authors' design, except that it will only include two lists (look negative and reappraise negative) of 10 images and the paradigm will be set up on Qualtrics and linked for use on mTurk. We will use the same standard to select and group images from OASIS; however, the specific images will differ from those used in the original study.

Link to paradigm: https://stanforduniversity.qualtrics.com/jfe/form/SV_1SKBE3Rkzs7qibz

### Procedure	

In the original study, participants "transcribed what they were thinking and feeling about the image into a textbox that appeared below the image" then "rated how they were feeling on a 7-point scale (1 Not bad at all to 7  Extremely bad)." They "completed 20 trials of each condition" and "reported their age, gender, race, and annual family income at the end of the survey."

"Mapping of list and condition was counterbalanced across participants."

We will precisely follow the original procedure as detailed above, except the mapping of the lists will be randomized as opposed to counterbalanced.

### Analysis Plan

The original researchers "computed each participant’s average negative affect rating for trials in each condition" and "used Pennebaker’s Linguistic Inquiry and Word Count (LIWC) program to analyze text entries for each trial" (Pennebaker et al., 2007). Text entries were proofread for spelling before analysis.

Nook et al. "focused linguistic analyses on (a) negative affect words (e.g., hurt, nasty, worried, sad, crying, annoyed), (b) positive affect words (e.g., love, nice, sweet, happy, laughing, cute), and (c) a composite linguistic measure of psychological distancing" (following Mehl, Robbins, & Holleran, 2012). To compute this measure, Nook et al. "z-scored use of first-person singular pronouns (e.g., I, me, my), present-tense verbs, articles (the, a, an), discrepancy words (e.g., would, could, should), and words of more than six letters across trials." They then "reverse-scored the z-scored frequencies of first-person singular pronouns, present-tense verbs, and discrepancy words by multiplying them by -1 and averaged these with the z-scored frequencies of articles and words of more than six letters for each trial." They "averaged this measure of linguistic distancing across trials within each condition for each participant."

To test whether people spontaneously distance their language when regulating emotions, one must determine whether participants' average use of linguistic distancing words (mean z-scored frequencies) differ significant across conditions. Nook et al. "used repeated-measures analyses of variance (ANOVAs) to test for significant differences across the three conditions." When significant effects emerged, they "conducted follow-up paired-samples t tests [the **key analysis of interest**] to assess for differences between the reappraise negative condition and the other two conditions."

To test whether the tendency to use more psychologically distant language when regulating emotions is associated with more successful regulation, one must determine whether there is a statistically significant correlation between the extent to which participants increase their use of words encoding psychological distance and the extent to which participants reduce their negative affect when regulating. Nook et al. "created a measure of reappraisal success for each participant by subtracting their average negative affect rating for images in the reappraise negative condition from their average rating for images in the look negative condition." They then "created analogous measures of how much each participant modulated their language when regulating their emotions by subtracting each participant’s average frequency of negative affect words, positive affect words, and linguistic distancing words in the look negative condition from their average use in the reappraise negative condition." Nook et al. "used Pearson’s correlations [the **key analysis of interest**] to test the hypotheses that higher reappraisal success scores would be associated with reduced use of negative affect words, increased use of positive affect words, and increased linguistic distancing."

We will precisely follow the original analysis plan as detailed above.

### Differences from Original Study

To ensure compliance with federal minimum wage, participants will be paid $1.50 for their time.

Our replication will only include two lists (look negative and reappraise negative) of 10 images and the paradigm will be set up on Qualtrics and linked for use on mTurk.

We will use the same standard to select and group images from OASIS; however, the specific images will differ from those used in the original study.

The mapping of our lists will be randomized as opposed to counterbalanced.

### Methods Addendum (Post Data Collection)

_You can comment this section out prior to final report with data collection._

#### Actual Sample

_Sample size, demographics, data exclusions based on rules spelled out in analysis plan_

#### Differences from pre-data collection methods plan

_Any differences from what was described as the original plan, or “none”._

## Results

### Data preparation
	
```{r load libraries, message=FALSE}
library(dplyr)
library(tidyr)
```

```{r import data}
csv <- read.csv("nook2017anonymized_data.csv")
```

```{r tidy data}
df.tidy <- csv %>% 
  
  # Remove unnecessary variables.
  select(-c(StartDate, EndDate, Status, Progress, Duration..in.seconds., Finished, RecordedDate, UserLanguage,starts_with("Q"), mTurk )) %>% 

  # Tidy data.
  unite("look1", look1, look1feeling, sep="__") %>% 
  unite("look1c", look1_c, look1feeling_c, sep="__") %>% 
  unite("look2", look2, look2feeling, sep="__") %>% 
  unite("look2c", look2_c, look2feeling_c, sep="__") %>% 
  unite("look3", look3, look3feeling, sep="__") %>% 
  unite("look3c", look3_c, look3feeling_c, sep="__") %>% 
  unite("look4", look4, look4feeling, sep="__") %>% 
  unite("look4c", look4_c, look4feeling_c, sep="__") %>% 
  unite("look5", look5, look5feeling, sep="__") %>% 
  unite("look5c", look5_c, look5feeling_c, sep="__") %>% 
  unite("look6", look6, look6feeling, sep="__") %>% 
  unite("look6c", look6_c, look6feeling_c, sep="__") %>% 
  unite("look7", look7, look7feeling, sep="__") %>% 
  unite("look7c", look7_c, look7feeling_c, sep="__") %>% 
  unite("look8", look8, look8feeling, sep="__") %>% 
  unite("look8c", look8_c, look8feeling_c, sep="__") %>% 
  unite("look9", look9, look9feeling, sep="__") %>% 
  unite("look9c", look9_c, look9feeling_c, sep="__") %>% 
  unite("look10", look10, look10feeling, sep="__") %>% 
  unite("look10c", look10_c, look10feeling_c, sep="__") %>% 
  unite("change1", change1, change1feeling, sep="__") %>% 
  unite("change1c", change1_c, change1feeling_c, sep="__") %>% 
  unite("change2", change2, change2feeling, sep="__") %>% 
  unite("change2c", change2_c, change2feeling_c, sep="__") %>% 
  unite("change3", change3, change3feeling, sep="__") %>% 
  unite("change3c", change3_c, change3feeling_c, sep="__") %>% 
  unite("change4", change4, change4feeling, sep="__") %>% 
  unite("change4c", change4_c, change4feeling_c, sep="__") %>% 
  unite("change5", change5, change5feeling, sep="__") %>% 
  unite("change5c", change5_c, change5feeling_c, sep="__") %>% 
  unite("change6", change6, change6feeling, sep="__") %>% 
  unite("change6c", change6_c, change6feeling_c, sep="__") %>% 
  unite("change7", change7, change7feeling, sep="__") %>% 
  unite("change7c", change7_c, change7feeling_c, sep="__") %>% 
  unite("change8", change8, change8feeling, sep="__") %>% 
  unite("change8c", change8_c, change8feeling_c, sep="__") %>% 
  unite("change9", change9, change9feeling, sep="__") %>% 
  unite("change9c", change9_c, change9feeling_c, sep="__") %>% 
  unite("change10", change10, change10feeling, sep="__") %>% 
  unite("change10c", change10_c, change10feeling_c, sep="__") %>% 
  
  pivot_longer(cols=c("look1", "look2", "look3", "look4", "look5", "look6", "look7", "look8", "look9", "look10", "change1", "change2", "change3", "change4", "change5", "change6", "change7", "change8", "change9", "change10", "look1c", "look2c", "look3c", "look4c", "look5c", "look6c", "look7c", "look8c", "look9c", "look10c","change1c", "change2c", "change3c", "change4c", "change5c", "change6c", "change7c", "change8c", "change9c", "change10c"),
               names_to = "condition",
               values_to = "response")

# Remove unnecessary rows.
df.clean <- df.tidy[!(df.tidy$DistributionChannel=="preview" |
                  df.tidy$response=="__" |
                  df.tidy$DistributionChannel=="Distribution Channel" |
                  df.tidy$age=="test" |
                  df.tidy$DistributionChannel=='{"ImportId":"distributionChannel"}' |
                  
                  # Exclude participants.
                  df.tidy$ResponseId=="R_2U8T4deIgQUKuSQ" |
                  df.tidy$ResponseId=="R_1K1GDNZ3Jxc6x9r"),]

df.wrangled <- df.clean %>% 
  separate(response, c("written_response", "rating"), sep ="__") %>% 
  select(-DistributionChannel)

# Rename observations.
df.wrangled$condition[df.wrangled$condition == c("look1", "look2", "look3", "look4", "look5")] <- "look"
df.wrangled$condition[df.wrangled$condition == c("look6", "look7", "look8", "look9", "look10")] <- "look"
df.wrangled$condition[df.wrangled$condition == c("look1c", "look2c", "look3c", "look4c", "look5c")] <- "look"
df.wrangled$condition[df.wrangled$condition == c("look6c", "look7c", "look8c", "look9c", "look10c")] <- "look"
df.wrangled$condition[df.wrangled$condition == c("change1", "change2", "change3", "change4", "change5")] <- "change"
df.wrangled$condition[df.wrangled$condition == c("change6", "change7", "change8", "change9", "change10")] <- "change"
df.wrangled$condition[df.wrangled$condition == c("change1c", "change2c", "change3c", "change4c", "change5c")] <- "change"
df.wrangled$condition[df.wrangled$condition == c("change6c", "change7c", "change8c", "change9c", "change10c")] <- "change"

df.wrangled$rating[df.wrangled$rating == "1 (Not bad at all)"] <- "1"
df.wrangled$rating[df.wrangled$rating == "7 (Extremely bad)"] <- "7"

View(df.wrangled)
```

```{r write csv file for LIWC processing}
write.csv(df.wrangled,"nook2017wrangled_data.csv", row.names = FALSE)
```

Here, wrangled data is manually edited for spelling errors and exporting issues (e.g. Qualtrics coding apostrophes as other icons). The edited csv was run through LIWC.

```{r import analyzed data}
df.LIWC <- read.csv("nook2017LIWC_analyzed_data.csv")
```

```{r prepare data for analyses}
df.new_variables <- df.LIWC %>% 
  rename(subject = Source..A.,
         age = Source..B., 
         gender = Source..C., 
         race = Source..D., 
         condition = Source..E., 
         written_response = Source..F., 
         rating = Source..G., 
         six_letter = Sixltr, 
         pronoun = i, 
         positive_affect = posemo, 
         negative_affect = negemo, 
         discrepancy = discrep) %>% 
  
  group_by(subject, condition) %>%
  mutate(pronoun_rz = -(pronoun - mean(pronoun, na.rm=TRUE))/sd(pronoun, na.rm=TRUE),
         verb_rz = -(verb - mean(verb, na.rm=TRUE))/sd(verb, na.rm=TRUE),
         article_z = (article - mean(article, na.rm=TRUE))/sd(article, na.rm=TRUE),
         discrepancy_rz = -(discrepancy - mean(discrepancy, na.rm=TRUE))/sd(discrepancy, na.rm=TRUE),
         six_letter_z = (six_letter - mean(six_letter, na.rm=TRUE))/sd(six_letter, na.rm=TRUE),
         mean_look_rating = ifelse(condition=="look", mean(rating), NA),
         mean_change_rating = ifelse(condition=="change", mean(rating), NA)) %>% 
  
  group_by(subject) %>%
  mutate(reappraisal = mean(mean_look_rating, na.rm = TRUE) - mean(mean_change_rating, na.rm = TRUE))

# Note: I am unsure how to treat the incomputable z-scores, so they may not be set to 0 in the final data analysis.
df.new_variables$pronoun_rz[df.new_variables$pronoun_rz == "NaN"] <- 0
df.new_variables$verb_rz[df.new_variables$verb_rz == "NaN"] <- 0
df.new_variables$article_z[df.new_variables$article_z == "NaN"] <- 0
df.new_variables$discrepancy_rz[df.new_variables$discrepancy_rz == "NaN"] <- 0
df.new_variables$six_letter_z[df.new_variables$six_letter_z == "NaN"] <- 0
  
df.linguistic_distancing <- df.new_variables %>% 
  mutate(linguistic_distancing = (pronoun_rz + verb_rz + article_z + discrepancy_rz + six_letter_z)/5,
         avg_linguistic_distancing = mean(linguistic_distancing))


View(df.linguistic_distancing)
```

### Confirmatory analysis

_The analyses as specified in the analysis plan._

```{r t-test}
t.test(formula = avg_linguistic_distancing ~ condition, data = df.linguistic_distancing)
```

```{r correlations}
cor_reappraisal_positive_affect <- cor.test(df.linguistic_distancing$reappraisal, 
                                            df.linguistic_distancing$positive_affect, 
                                            method = "pearson")
cor_reappraisal_positive_affect

cor.reappraisal_negative_affect <- cor.test(df.linguistic_distancing$reappraisal, 
                                            df.linguistic_distancing$negative_affect, 
                                            method = "pearson")
cor.reappraisal_negative_affect

cor.reappraisal_linguistic_distancing <- cor.test(df.linguistic_distancing$reappraisal, 
                                                  df.linguistic_distancing$linguistic_distancing, 
                                                  method = "pearson")
cor.reappraisal_linguistic_distancing
```

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

_Any follow-up analyses desired (not required)._

## Discussion

### Summary of Replication Attempt

_Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result._  

### Commentary

_Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long._
